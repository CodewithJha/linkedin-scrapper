name: Daily LinkedIn Scrape

on:
  # Run every day at 6:00 AM UTC (11:30 AM IST)
  schedule:
    - cron: '0 6 * * *'
  
  # Allow manual trigger with custom inputs
  workflow_dispatch:
    inputs:
      keywords:
        description: 'Job keywords (e.g., data engineer)'
        required: false
        default: ''
      location:
        description: 'Location (e.g., India, Remote)'
        required: false
        default: ''
      results:
        description: 'Number of results (10-100)'
        required: false
        default: ''
      timePosted:
        description: 'Time filter (past24h, pastWeek, any)'
        required: false
        default: ''
      startupsOnly:
        description: 'Filter to startup-likely companies only'
        required: false
        default: 'false'

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      
      - name: Install dependencies
        run: npm ci
      
      - name: Install Playwright browsers
        run: npx playwright install chromium
      
      - name: Create runtime config
        run: |
          # Start with default config
          if [ -f config.example.json ]; then
            cp config.example.json config.json
          else
            echo '{}' > config.json
          fi
          
          # Override with workflow inputs if provided
          KEYWORDS="${{ github.event.inputs.keywords }}"
          LOCATION="${{ github.event.inputs.location }}"
          RESULTS="${{ github.event.inputs.results }}"
          TIME_POSTED="${{ github.event.inputs.timePosted }}"
          STARTUPS_ONLY="${{ github.event.inputs.startupsOnly }}"
          
          # Use Node.js to merge config
          node -e "
            const fs = require('fs');
            let config = {};
            try { config = JSON.parse(fs.readFileSync('config.json', 'utf-8')); } catch {}
            
            if ('$KEYWORDS') config.keywords = '$KEYWORDS';
            if ('$LOCATION') config.location = '$LOCATION';
            if ('$RESULTS' && !isNaN(parseInt('$RESULTS'))) config.resultsPerSession = parseInt('$RESULTS');
            if ('$TIME_POSTED') config.timePosted = '$TIME_POSTED';
            if ('$STARTUPS_ONLY' === 'true') config.startupsOnly = true;
            
            // Always set headless and output
            config.headless = true;
            config.outputDir = 'out';
            
            fs.writeFileSync('config.json', JSON.stringify(config, null, 2));
            console.log('Config:', config);
          "
      
      - name: Run scraper
        run: npm run scrape:once
        env:
          LINKEDIN_COOKIE: ${{ secrets.LINKEDIN_COOKIE }}
      
      - name: Commit and push new CSVs
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          # Add any new/modified files in out/ and data/
          git add out/*.csv data/seen-jobs.json || true
          
          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No new jobs found, nothing to commit"
          else
            KEYWORDS="${{ github.event.inputs.keywords }}"
            LOCATION="${{ github.event.inputs.location }}"
            MSG="ðŸ¤– Scrape: $(date +'%Y-%m-%d %H:%M')"
            if [ -n "$KEYWORDS" ]; then
              MSG="$MSG | $KEYWORDS"
            fi
            if [ -n "$LOCATION" ]; then
              MSG="$MSG in $LOCATION"
            fi
            git commit -m "$MSG"
            # Pull latest changes and rebase before pushing
            git pull --rebase origin main
            git push
          fi
